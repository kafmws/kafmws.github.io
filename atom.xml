<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>kafm&#39; blog</title>
  
  <subtitle>kafm&#39; blog</subtitle>
  <link href="https://kafm.eu.org/atom.xml" rel="self"/>
  
  <link href="https://kafm.eu.org/"/>
  <updated>2026-02-06T19:13:52.000Z</updated>
  <id>https://kafm.eu.org/</id>
  
  <author>
    <name>kafm</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LLM 结构化输出原理、实现方式及实践</title>
    <link href="https://kafm.eu.org/notes/LLM/LLM%20%E7%BB%93%E6%9E%84%E5%8C%96%E8%BE%93%E5%87%BA%E5%8E%9F%E7%90%86%E3%80%81%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%E5%8F%8A%E5%AE%9E%E8%B7%B5/"/>
    <id>https://kafm.eu.org/notes/LLM/LLM%20%E7%BB%93%E6%9E%84%E5%8C%96%E8%BE%93%E5%87%BA%E5%8E%9F%E7%90%86%E3%80%81%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%E5%8F%8A%E5%AE%9E%E8%B7%B5/</id>
    <published>2026-02-02T18:43:54.000Z</published>
    <updated>2026-02-06T19:13:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="结构化输出的意义"><a class="markdownIt-Anchor" href="#结构化输出的意义"></a> 结构化输出的意义</h2><p>LLM 是输出不可控的概率模型，而结构化输出则增强输出的可控性。<br />有了这种可控性，LLM 就具备了一个程序的交互 interface 可以稳定地接入上下游程序，比如调用工具</p><h2 id="结构化输出的原理及实现方式"><a class="markdownIt-Anchor" href="#结构化输出的原理及实现方式"></a> 结构化输出的原理及实现方式</h2><p>实现格式化输出通常有以下几种手段，</p><ul><li>提示词工程<br />通过指令 “只输出JSON”，few-shot 示例等提示词技巧引导模型进行格式化输出</li><li>后处理<br />instruct 等库对模型输出进行后处理，进行提取或修复等。包括解析后反馈错误重新生成等。</li><li>约束解码<br />在选择 token 时用 mask 机制直接排除不符合输出格式的候选 token，在有限输出空间内选择 token。<br /><code>{&quot;key&quot;: value}</code>，从仅让模型预测格式串中的<code>value</code>部分，到约束<code>value</code>的格式/类型，约束解码都可以保证可靠的格式化输出。<br />再进一步，通过等价的状态机进行约束解码，可以进行任意的格式化输出。约束解码是结构化输出的主要工程手段。</li><li>SFT / RL<br />通过后训练手段形成输出偏好，使模型遵循一定模式。可靠性高，是结构化输出的理论保障、能力来源。</li></ul><p>当前许多模型厂商从 SFT/RL 的角度增强模型的格式化输出能力，并从推理端实现约束解码（例如Ollama），从而提供可靠的结构化输出 API。</p><h2 id="工具调用与结构化输出先有鸡还是先有蛋"><a class="markdownIt-Anchor" href="#工具调用与结构化输出先有鸡还是先有蛋"></a> 工具调用与结构化输出，先有鸡还是先有蛋</h2><p>依靠结构化输出的结果，我们才能稳定地处理工具调用<br />然而，如果模型能调用给定的工具，那么任意输出格式都可以包装成工具，通过 LLM 填写调用参数，也就实现了指定格式的输出<br />看起来两者相互成就，已经变成了鸡生蛋，蛋生鸡的问题<br />许多厂商都强调模型有 tool use/ function call 能力，却不强调模型支持结构化输出，可能相比于输出“特定”格式，输出函数参数更好构造训练数据以及验证吧</p><h2 id="本地模型在-langchain-中的结构化输出"><a class="markdownIt-Anchor" href="#本地模型在-langchain-中的结构化输出"></a> 本地模型在 langchain 中的结构化输出</h2><p>本文在 langchain 以下生态中进行讨论（2026-02-02）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">langchain-core            1.2.7</span><br><span class="line">langchain-openai          1.1.7</span><br><span class="line">langchain-ollama          1.0.1</span><br></pre></td></tr></table></figure><p>以 <code>YesNoJudge</code> 类的 JSON 格式，搭配不同 <code>with_structured_output()</code> 的格式化输出方式（即<code>method</code>参数）测试结构化输出表现<br /><code>method</code> 参数可选项有 <code>json_schema</code>, <code>function_calling</code> 和 <code>json_mode</code>，其中<code>json_mode</code>因无法给出确定格式的 JSON ，需要后处理，不推荐使用。（来源：<a class="link"   href="https://platform.openai.com/docs/guides/structured-outputs#json-mode%EF%BC%89" >https://platform.openai.com/docs/guides/structured-outputs#json-mode）<i class="fas fa-external-link-alt"></i></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">YesNoJudge</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;return judge result from the context with this function.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    yes_or_no: <span class="type">Literal</span>[<span class="string">&quot;yes&quot;</span>, <span class="string">&quot;no&quot;</span>] = Field(</span><br><span class="line">        description=<span class="string">&quot;The answer should be either &#x27;yes&#x27; or &#x27;no&#x27;&quot;</span></span><br><span class="line">    )</span><br><span class="line">    explanation: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">judge = ChatOpenAI(</span><br><span class="line">            model=model_name, temperature=<span class="number">0.6</span>, streaming=<span class="literal">True</span>, **fast_judge_config</span><br><span class="line">        )</span><br><span class="line">        judge = judge.with_structured_output(</span><br><span class="line">            schema=YesNoJudge, method=<span class="string">&quot;function_calling&quot;</span>, strict=<span class="literal">True</span>, include_raw=<span class="literal">True</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure><img                           lazyload                       alt="image"                       data-src="https://cdn.jsdelivr.net/gh/kafmws/pictures/notes/20260201223737.png"                         alt="json_schema 方式下在 prompt 中提示按JSON格式输出后，gpt-oss:20b 仍调用不存在的工具进行格式化输出"                 >图：指定`json_schema` 方式，在 prompt 中提示按JSON格式输出后，gpt-oss:20b 仍调用不存在的工具进行格式化输出<img                           lazyload                       alt="image"                       data-src="https://cdn.jsdelivr.net/gh/kafmws/pictures/notes/20260202030229.png"                         alt="function_calling 方式下在 prompt 中提示按JSON格式输出后，qwen3:4b 会在内容中输出 json，不调用工具"                 >图：指定 `function_calling` 方式，在 prompt 中提示按JSON格式输出后，qwen3:4b 会在内容中输出 json，不调用工具<img                           lazyload                       alt="image"                       data-src="https://cdn.jsdelivr.net/gh/kafmws/pictures/notes/20260202030504.png"                         alt="function_calling 方式下去除json prompt，qwen3:4b 仍倾向于遵循指令在内容中输出结果，不调用工具"                 >图：指定 `function_calling` 方式，去除json prompt，qwen3:4b 仍倾向于遵循指令在内容中输出结果，不调用工具<img                           lazyload                       alt="image"                       data-src="https://cdn.jsdelivr.net/gh/kafmws/pictures/notes/20260202031010.png"                         alt="20260202031010"                 >图：指定 `function_calling` 方式，去除指令式prompt，qwen3:4b 有概率调用工具返回结果<p><strong>可见 Qwen 模型更重视指令遵循，而 GPT 偏向于使用工具</strong></p><h3 id="langchain-中-with_structured_output-的实现简析"><a class="markdownIt-Anchor" href="#langchain-中-with_structured_output-的实现简析"></a> langchain 中 with_structured_output() 的实现简析</h3><p>不同 backend 生态包的 with_structured_output() 实现不同，<br />对于langchan-openai：</p><ul><li>function_calling 把 JSON schema 作为 tool 传入，并填充 tool_choice 参数，强制 LLM 使用工具输出工具调用的 schema<br />相当于把工具调用能力转化为结构化输出能力</li><li>json_schema 方式则是对输出内容进行解析，该参数</li><li>json_mode 建议配合 prompt 使用，全靠模型发挥<br />而 create_agnet API 中 response_format 也是直接调用 LLM 后端的结构化输出能力</li></ul><h2 id="langchain-langchain-openai-ollama-后端结构化输出踩的坑"><a class="markdownIt-Anchor" href="#langchain-langchain-openai-ollama-后端结构化输出踩的坑"></a> langchain + langchain-openai + Ollama 后端结构化输出踩的坑</h2><p>为什么用 langchain-openai 不用 langchain-ollama，因为想着兼容 OpenAI API 会好一些<br />甚至用 langchain-ollama + Ollama 还一直出现 502 报错。。openai / curl 均正常</p><ul><li><p>看似兼容，实则不兼容<br />如 langchain[openai, ollama] 中 chat_models 的 with_structured_output() 实现不一致<br />且将 Ollama 作为后端使用 langchain-openai 时 with_structured_output 的表现和模型高度相关，需根据模型调优 method 参数<br />实际支持多个后端仍需要进行适配</p></li><li><p>多处实现，效果不一<br />如 langchain[openai, ollama] 中 chat_models 的 with_structured_output() 与 from langchain.agents import create_agent 的 response_format 都用于结构化输出<br />然而可靠性不同</p></li><li><p>过度封装<br />源码里到处是消除警告的类型不匹配 # type: ignore<br />MessageState 的消息 messages 传入 AgentState 的 messages 提示类型不匹配，有点搞笑</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;结构化输出的意义&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#结构化输出的意义&quot;&gt;&lt;/a&gt; 结构化输出的意义&lt;/h2&gt;
&lt;p&gt;LLM 是输出不可控的概率模型，而结构化输出则增强输出的可控性。&lt;br /&gt;
有了这种可控性，LLM 就具备</summary>
      
    
    
    
    <category term="notes" scheme="https://kafm.eu.org/categories/notes/"/>
    
    
    <category term="LLM杂七杂八" scheme="https://kafm.eu.org/tags/LLM%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
  </entry>
  
  <entry>
    <title>龙潭公园里的鹅和鸭们日子过得无忧无虑</title>
    <link href="https://kafm.eu.org/fragments/Fragments/%E9%BE%99%E6%BD%AD%E5%85%AC%E5%9B%AD%E9%87%8C%EF%BC%8C%E9%B9%85%E5%92%8C%E9%B8%AD%E4%BB%AC%E6%97%A5%E5%AD%90%E8%BF%87%E5%BE%97%E6%97%A0%E5%BF%A7%E6%97%A0%E8%99%91/"/>
    <id>https://kafm.eu.org/fragments/Fragments/%E9%BE%99%E6%BD%AD%E5%85%AC%E5%9B%AD%E9%87%8C%EF%BC%8C%E9%B9%85%E5%92%8C%E9%B8%AD%E4%BB%AC%E6%97%A5%E5%AD%90%E8%BF%87%E5%BE%97%E6%97%A0%E5%BF%A7%E6%97%A0%E8%99%91/</id>
    <published>2025-12-21T04:43:54.000Z</published>
    <updated>2026-02-05T20:37:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>龙潭公园里的鹅和鸭们日子过得无忧无虑呐。</p><p>潭水表面大都结着冰，沿潭水绕行一圈，每个鹅鸭聚集的岸边都有人在喂食，有馒头，有不知名的食儿，甚至还有个眼镜哥带一大串葡萄来喂。<br />馒头屑浮在水面上，鸭子悠悠游来伸脖一衔便进肚，而葡萄都沉入水中。<br />有鸭子站在冰面上了，就有葡萄滚落周围，这下总能吃了吧！<br />可葡萄圆滚滚的，这黑绿头的鸭子用窄窄扁扁的嘴衔几次掉落几次，只有极少数葡萄完成了使命。<br />眼镜哥毫不在乎，在冰上、在水里，吃不吃、吃不吃得着无关紧要，喂食本是一种过程。</p><p>继续往前走，又一大群鸭，和拎一大袋馒头的大姨。这边鸭也是游水时偷闲吃两口，但成群结队游向岸边还是给足了大姨面子。<br />有旁人说道，一清早就有人喂过了！<br />怨不得鸭吃得糊弄，原来是尝个咸淡，看合不合胃口。</p><p>龙潭公园里，鹅和鸭们日子过得无忧无虑呐。</p><p>25年12月21日</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;龙潭公园里的鹅和鸭们日子过得无忧无虑呐。&lt;/p&gt;
&lt;p&gt;潭水表面大都结着冰，沿潭水绕行一圈，每个鹅鸭聚集的岸边都有人在喂食，有馒头，有不知名的食儿，甚至还有个眼镜哥带一大串葡萄来喂。&lt;br /&gt;
馒头屑浮在水面上，鸭子悠悠游来伸脖一衔便进肚，而葡萄都沉入水中。&lt;br /&gt;
</summary>
      
    
    
    
    <category term="fragments" scheme="https://kafm.eu.org/categories/fragments/"/>
    
    
    <category term="随笔" scheme="https://kafm.eu.org/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>
